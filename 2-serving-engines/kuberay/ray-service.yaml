apiVersion: ray.io/v1
kind: RayService
metadata:
  name: rayservice-sample
spec:
  serveConfigV2: |
    applications:
      - name: llm_api
        route_prefix: "/"
        import_path: llm_api:deployment
        deployments:
          - name: LLMService
            num_replicas: 1
            ray_actor_options:
              num_cpus: 1
              num_gpus: 0

  rayClusterConfig:
    headGroupSpec:
      rayStartParams:
        dashboard-host: "0.0.0.0"
      template:
        spec:
          containers:
          - name: ray-head
            image: rayproject/ray:2.9.0-py310
            ports:
            - containerPort: 6379
              name: gcs
            - containerPort: 8265
              name: dashboard
            - containerPort: 10001
              name: client
            - containerPort: 8000
              name: serve
            resources:
              limits:
                cpu: "2"
                memory: "4G"
              requests:
                cpu: "2"
                memory: "4G"
            volumeMounts:
            - mountPath: /tmp/ray
              name: ray-logs
            - mountPath: /tmp/model_cache
              name: model-cache
            - mountPath: /home/ray/llm_api.py
              name: llm-api-script
              subPath: llm_api.py
            env:
            - name: MODEL_URL
              value: "meta-llama/Llama-3.1-8B-Instruct"
            - name: HF_TOKEN
              valueFrom:
                secretKeyRef:
                  name: hf-token
                  key: token
          volumes:
          - name: ray-logs
            emptyDir: {}
          - name: model-cache
            emptyDir:
              medium: Memory
              sizeLimit: "20Gi"
          - name: llm-api-script
            configMap:
              name: llm-api-script
              defaultMode: 0644

    workerGroupSpecs:
      - groupName: small-group
        replicas: 1
        minReplicas: 1
        maxReplicas: 10
        rayStartParams: {}
        template:
          spec:
            containers:
            - name: ray-worker
              image: rayproject/ray:2.9.0-py310
              resources:
                limits:
                  cpu: "2"
                  memory: "4G"
                requests:
                  cpu: "2"
                  memory: "4G"
              volumeMounts:
              - mountPath: /tmp/ray
                name: ray-logs
              - mountPath: /tmp/model_cache
                name: model-cache
              - mountPath: /home/ray/llm_api.py
                name: llm-api-script
                subPath: llm_api.py
              env:
              - name: MODEL_URL
                value: "meta-llama/Llama-3.1-8B-Instruct"
              - name: HF_TOKEN
                valueFrom:
                  secretKeyRef:
                    name: hf-token
                    key: token
            volumes:
            - name: ray-logs
              emptyDir: {}
            - name: model-cache
              emptyDir:
                medium: Memory
                sizeLimit: "20Gi"
            - name: llm-api-script
              configMap:
                name: llm-api-script
                defaultMode: 0644