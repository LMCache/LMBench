servingEngineSpec:
  runtimeClassName: ""
  modelSpec:
  - name: "llama1"
    repository: "lmcache/vllm-openai"
    tag: "2025-05-27-v1"
    modelURL: "meta-llama/Llama-3.1-70B-Instruct"
    replicaCount: 1
    requestCPU: 6
    requestMemory: "70Gi"
    requestGPU: 2
    pvcStorage: "50Gi"
    vllmConfig:
      enablePrefixCaching: true
      maxModelLen: 32000
      v1: 1
      extraArgs:
        - "--tensor-parallel-size"
        - "2"
        - "CUDA_VISIBLE_DEVICES=0,1"

    lmcacheConfig:
      enabled: true
      cudaVisibleDevices: "0,1"
      cpuOffloadingBufferSize: "60"
      enableController: true
      instanceId: "default1"
      controllerPort: "9000"
      workerPort: 8001

    env:
      - name: LMCACHE_LOG_LEVEL
        value: "DEBUG"
    hf_token: <YOUR_HF_TOKEN>

  # - name: "llama2"
  #   repository: "lmcache/vllm-openai"
  #   tag: "2025-05-27-v1"
  #   modelURL: "meta-llama/Llama-3.1-70B-Instruct"
  #   replicaCount: 1
  #   requestCPU: 6
  #   requestMemory: "30Gi"
  #   requestGPU: 2
  #   pvcStorage: "50Gi"
  #   vllmConfig:
  #     enablePrefixCaching: true
  #     maxModelLen: 32000
  #     v1: 1
  #     extraArgs:
  #       - "--tensor-parallel-size"
  #       - "2"

  #   lmcacheConfig:
  #     enabled: true
  #     cudaVisibleDevices: "2,3"
  #     cpuOffloadingBufferSize: "60"
  #     enableController: true
  #     instanceId: "default2"
  #     controllerPort: "9000"
  #     workerPort: 8002

  #   env:
  #     - name: LMCACHE_LOG_LEVEL
  #       value: "DEBUG"
  #   hf_token: <YOUR_HF_TOKEN>

  # - name: "llama3"
  #   repository: "lmcache/vllm-openai"
  #   tag: "2025-05-27-v1"
  #   modelURL: "meta-llama/Llama-3.1-70B-Instruct"
  #   replicaCount: 1
  #   requestCPU: 6
  #   requestMemory: "70Gi"
  #   requestGPU: 2
  #   pvcStorage: "50Gi"
  #   vllmConfig:
  #     enablePrefixCaching: true
  #     maxModelLen: 32000
  #     v1: 1
  #     extraArgs:
  #       - "--tensor-parallel-size"
  #       - "2"

  #   lmcacheConfig:
  #     enabled: true
  #     cudaVisibleDevices: "4,5"
  #     cpuOffloadingBufferSize: "60"
  #     enableController: true
  #     instanceId: "default3"
  #     controllerPort: "9000"
  #     workerPort: 8003

  #   env:
  #     - name: LMCACHE_LOG_LEVEL
  #       value: "DEBUG"

  #   hf_token: <YOUR_HF_TOKEN>
  # - name: "llama4"
  #   repository: "lmcache/vllm-openai"
  #   tag: "2025-05-27-v1"
  #   modelURL: "meta-llama/Llama-3.1-70B-Instruct"
  #   replicaCount: 1
  #   requestCPU: 6
  #   requestMemory: "70Gi"
  #   requestGPU: 2
  #   pvcStorage: "50Gi"
  #   vllmConfig:
  #     enablePrefixCaching: true
  #     maxModelLen: 32000
  #     v1: 1
  #     extraArgs:
  #       - "--tensor-parallel-size"
  #       - "2"

  #   lmcacheConfig:
  #     enabled: true
  #     cudaVisibleDevices: "6,7"
  #     cpuOffloadingBufferSize: "60"
  #     enableController: true
  #     instanceId: "default4"
  #     controllerPort: "9000"
  #     workerPort: 8004

  #   env:
  #     - name: LMCACHE_LOG_LEVEL
  #       value: "DEBUG"
  #   hf_token: <YOUR_HF_TOKEN>

routerSpec:
  repository: "lmcache/lmstack-router"
  tag: "kvaware-latest"
  resources:
    requests:
      cpu: "1"
      memory: "2G"
    limits:
      cpu: "1"
      memory: "2G"
  routingLogic: "kvaware"
  lmcacheControllerPort: 9000
  hf_token: <YOUR_HF_TOKEN>
  sessionKey: "x-user-id"