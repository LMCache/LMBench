Name: OSS-Comparisons-4xH100-Qwen3-32B-TP2

# Suggested Infrastructure (in run-bench.yaml)
# 1-infrastructure:
#   Location: Local-Flat
#   Use 8x H100s or A100s (Make sure to set the correct acceleratorType in RayServe)
#   All deployments below use 4x Qwen-32B with tensor parallelism 2

Serving:
#   - Helm-ProductionStack:
#       helmConfigSelection: open-source/comparison-baseline.yaml
#       hf_token: <YOUR_HF_TOKEN>
#       modelURL: Qwen/Qwen3-32B
#   - SGLang:
#       scriptName: comparison-baseline.sh
#       modelURL: Qwen/Qwen3-32B
  - RayServe:
      scriptName: comparison-baseline.py
      acceleratorType: H100
      modelURL: Qwen/Qwen3-32B
#   - LLM-D:
#       configSelection: comparison-baseline.yaml
#       modelURL: Qwen/Qwen3-32B
#       hf_token: <YOUR_HF_TOKEN>
  - Dynamo:
      configSelection: comparison-baseline.yaml
      modelURL: Qwen/Qwen3-14B
      apiType: chat

Workload:

  LMCacheSynthetic:
    # long input short output:
    - NUM_USERS_WARMUP: 60
      NUM_USERS: 60
      NUM_ROUNDS: 20
      SYSTEM_PROMPT: 1000
      CHAT_HISTORY: 20000
      ANSWER_LEN: 100
      QPS: [0.5, 1, 2, 3, 4, 5, 6]
      USE_SHAREGPT: false