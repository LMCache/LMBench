Name: <NAME_OF_BENCHMARK_SUITE> # e.g. layerwise, routing, pd, etc.

# 2-serving-engines/
Serving:
  # Choose one or more of the following. You can have multiple of each as well.
  # NOTE: SGLang and RayServe require Local-Flat infrastructure in run-bench.yaml

  # use the latest helm repository from production-stack
  - Helm-ProductionStack:
      # this will start in the `2-serving-engines/helm-production-stack/helm_configurations/` directory
      # examples:
      # - helmConfigSelection: routing/1-kvaware.yaml
      # - helmConfigSelection: basic/simple-vllm.yaml
      helmConfigSelection: <NAME_OF_HELM_CONFIG_FILENAME>

      # You do NOT need to specify hf_token if you are using LMCacheGKE
      hf_token: <YOUR_HF_TOKEN>
      # Please make sure you are using the correct modelURL for the helm config you are using so that the workload generator can select the correct model
      modelURL: <MODEL_USED_IN_HELM_CONFIG>

  # Any novel lmcache or production stack features can be tested here (before they are helm released)
  - Direct-ProductionStack:
      # this will start in the `2-serving-engines/direct-production-stack/kubernetes_configurations/` directory
      # generally this will be some modification after directly helm rendering the producttion stack helm chart
      # examples:
      # - kubernetesConfigSelection: routing/lmcache_roundrobin.yaml
      # - kubernetesConfigSelection: routing/lmcache_random.yaml
      # - kubernetesConfigSelection: routing/lmcache_roundrobin_with_prefix_caching.yaml
      kubernetesConfigSelection: <NAME_OF_K8S_CONFIG_FILENAME>

      # You do NOT need to specify hf_token if you are using LMCacheGKE
      hf_token: <YOUR_HF_TOKEN>
      # Please make sure you are using the correct modelURL for the kubernetes config you are using so that the workload generator can select the correct model
      modelURL: <MODEL_USED_IN_K8S_CONFIG>

  # Option 3: SGLang (Local-Flat infrastructure ONLY)
  - SGLang:
      scriptName: basic-deployment.sh  # Script in 2-serving-engines/sglang/
      modelURL: meta-llama/Llama-3.1-8B-Instruct
      # NOTE: SGLang requires Local-Flat infrastructure. 
      # The script reads HF_TOKEN directly from environment variable.

  # Option 4: RayServe (Local-Flat infrastructure ONLY)
  - RayServe:
      scriptName: basic-deployment.sh  # Script in 2-serving-engines/rayserve/
      acceleratorType: H100  # e.g., H100, A100, V100, T4
      modelURL: meta-llama/Llama-3.1-70B-Instruct
      # NOTE: RayServe requires Local-Flat infrastructure.
      # The script reads HF_TOKEN directly from environment variable.
      # Example: ./basic-deployment.sh H100

  # Option 5: Dynamo
  - Dynamo:
      # Coming soon...

  # Option 6: LLM-D
  - LLM-D:
      # Coming soon...

# 3-workloads/
Workload:
  # Multiple workloads can be specified and they will all be run.

  ShareGPT:
    - LIMIT: 1000
      MIN_ROUNDS: 10
      START_ROUND: 0
      QPS: [1.34, 2]

  LMCacheSynthetic:
    - NUM_USERS_WARMUP: 650
      NUM_USERS: 350
      NUM_ROUNDS: 20
      SYSTEM_PROMPT: 0 # shared between users
      CHAT_HISTORY: 20000 # unique for each user
      ANSWER_LEN: 1000
      QPS: [0.7]
      USE_SHAREGPT: false

    # commonly used combinations:

    # long input long output:
    - NUM_USERS_WARMUP: 750
      NUM_USERS: 350
      NUM_ROUNDS: 20
      SYSTEM_PROMPT: 0
      CHAT_HISTORY: 20000
      ANSWER_LEN: 1000
      QPS: [0.7]
      USE_SHAREGPT: false
    # long input short output:
    - NUM_USERS_WARMUP: 20
      NUM_USERS: 15
      NUM_ROUNDS: 20
      SYSTEM_PROMPT: 1000
      CHAT_HISTORY: 20000
      ANSWER_LEN: 100
      QPS: [0.1]
      USE_SHAREGPT: false

    # short input short output:
    - NUM_USERS_WARMUP: 400
      NUM_USERS: 320
      NUM_ROUNDS: 20
      SYSTEM_PROMPT: 0
      CHAT_HISTORY: 256
      ANSWER_LEN: 20
      QPS: [15]
      USE_SHAREGPT: false

  Mooncake:
    - NUM_ROUNDS: 20
      SYSTEM_PROMPT: 0
      CHAT_HISTORY: 256
      ANSWER_LEN: 20
      QPS: [1]

  Agentic:
    - NUM_USERS_WARMUP: 100
      NUM_AGENTS: 10
      NUM_ROUNDS: 20
      SYSTEM_PROMPT: 0
      CHAT_HISTORY: 256
      ANSWER_LEN: 20
      NEW_USER_INTERVALS: [1]

  Random:
    # Random workload generates completely random prompts with no shared prefix
    # This is ideal for testing store-heavy workloads in LMCache
    - NUM_USERS: 100
      NUM_ROUNDS: 10
      PROMPT_LEN: 200  # Length of random prompts (in words)
      ANSWER_LEN: 100  # Length of the answer (max tokens)
      QPS: [1.0, 2.0]

    # commonly used combinations:

    # short random prompts:
    - NUM_USERS: 50
      NUM_ROUNDS: 15
      PROMPT_LEN: 50
      ANSWER_LEN: 50
      QPS: [5.0]

    # long random prompts:
    - NUM_USERS: 200
      NUM_ROUNDS: 20
      PROMPT_LEN: 500
      ANSWER_LEN: 200
      QPS: [0.5]

  VLLMBenchmark:
    # Super minimal configuration - only required parameters
    - BACKEND: vllm
      DATASET_NAME: random
      NUM_PROMPTS: 100
      REQUEST_RATES: [1.0]

    # VLLMBenchmark workload uses the vLLM benchmark serving script
    # This provides comprehensive benchmarking capabilities with various datasets
    - BACKEND: vllm  # Backend type: vllm, openai, etc.
      DATASET_NAME: random  # Dataset: sharegpt, random, sonnet, hf, custom
      DATASET_PATH: ""  # Path to dataset file (leave empty for built-in datasets)
      NUM_PROMPTS: 1000  # Number of prompts to process
      REQUEST_RATES: [1.0, 2.0, 5.0]  # Request rates to test (requests per second)

      # Optional sampling parameters
      TEMPERATURE: 0.0  # Temperature for sampling (0.0 for greedy)
      TOP_P: 0.9  # Top-p sampling parameter
      TOP_K: 50   # Top-k sampling parameter
      MAX_TOKENS: 256  # Maximum tokens to generate

      # Optional benchmark parameters
      BURSTINESS: 1.0  # Burstiness factor (1.0 = Poisson process)
      SEED: 0  # Random seed for reproducibility
      DISABLE_TQDM: true  # Disable progress bar
      IGNORE_EOS: false  # Ignore end-of-sequence tokens

      # For Random dataset:
      RANDOM_INPUT_LEN: 1024   # Input length for random prompts
      RANDOM_OUTPUT_LEN: 128   # Output length for random prompts
      RANDOM_RANGE_RATIO: 0.0  # Range ratio for length variation

    # Example with Sonnet dataset (requires sonnet.txt file)
    - BACKEND: vllm
      DATASET_NAME: sonnet
      DATASET_PATH: "3-workloads/vllm-benchmark-serving/sonnet.txt"  # Path to sonnet file
      NUM_PROMPTS: 500
      REQUEST_RATES: [0.5, 1.0]
      SONNET_INPUT_LEN: 550    # Input length for sonnet prompts
      SONNET_OUTPUT_LEN: 150   # Output length for sonnet prompts
      SONNET_PREFIX_LEN: 200   # Prefix length for sonnet prompts
      TEMPERATURE: 0.7
      SEED: 42

    # Example with HuggingFace dataset (MT-Bench is non-multimodal)
    - BACKEND: vllm
      DATASET_NAME: hf
      DATASET_PATH: "philschmid/mt-bench"  # HuggingFace dataset ID
      NUM_PROMPTS: 200
      REQUEST_RATES: [1.0, 2.0]
      TEMPERATURE: 0.0
      MAX_TOKENS: 256


