Name: <NAME_OF_BENCHMARK_SUITE> # e.g. layerwise, routing, pd, etc.

# 2-serving-engines/
Serving:
  # Choose one or more of the following. You can have multiple of each as well.

  # use the latest helm repository from production-stack
  - Helm-ProductionStack:
      vLLM-Version: 0
      enablePrefixCaching: false # vllm v1 specific only (no prefix caching in v0)
      useLMCache: false
      modelURL: meta-llama/Llama-3.1-8B-Instruct
      replicaCount: 1 # PLEASE make sure that replicaCount x numGPUs <= numClusterGPUs
      numGPUs: 1 # PLEASE make sure that replicaCount x numGPUs <= numClusterGPUs
      numCPUs: 4 # PLEASE look at the vCPU limits in the comment above (try to keep 12 or below)
      tensorParallelSize: 1 # please make sure tensorParallelSize <= numGPUs (this is the number of GPUs per replica)

      # You do NOT need to specify hf_token if you are using LMCacheGKE
      hf_token: <YOUR_HF_TOKEN>
      maxModelLen: 16384

  # Any novel lmcache or production stack features can be tested here (before they are helm released)
  - Direct-ProductionStack:
      # this will start in the `2-serving-engines/direct-production-stack/kubernetes_configurations/` directory
      # generally this will be some modification after directly helm rendering the producttion stack helm chart
      # examples:
      # - kubernetesConfigSelection: routing/lmcache_roundrobin.yaml
      # - kubernetesConfigSelection: routing/lmcache_random.yaml
      # - kubernetesConfigSelection: routing/lmcache_roundrobin_with_prefix_caching.yaml
      kubernetesConfigSelection: <NAME_OF_K8S_CONFIG_FILENAME>

      # You do NOT need to specify hf_token if you are using LMCacheGKE
      hf_token: <YOUR_HF_TOKEN>
      # Please make sure you are using the correct modelURL for the kubernetes config you are using so that the workload generator can select the correct model
      modelURL: <MODEL_USED_IN_K8S_CONFIG>

  # Option 3: SGLang
  - SGLang:
      modelURL: meta-llama/Llama-3.1-8B-Instruct

      # You do NOT need to specify hf_token if you are using LMCacheGKE
      hf_token: <YOUR_HF_TOKEN>
      replicaCount: 1
      numGPUs: 1 # number of GPUs per replica
      numCPUs: 10 # number of CPUs per replica
      requestMemory: "50Gi" # memory request per replica
      shmSize: "20Gi" # shared memory size
      cacheSize: "50Gi" # size of the HuggingFace cache volume
      contextLength: 32768 # context length for the model
      tensorParallelSize: 1 # tensor parallel size for model distribution across GPUs

  # Option 4: Dynamo
  - Dynamo:
      # Coming soon...

  # Option 5: KubeRay (Broken ATM)
  - KubeRay:
      # Coming soon...

# 3-workloads/
Workload:
  # Multiple workloads can be specified and they will all be run.

  ShareGPT:
    - LIMIT: 1000
      MIN_ROUNDS: 10
      START_ROUND: 0
      QPS: [1.34, 2]

  LMCacheSynthetic:
    - NUM_USERS_WARMUP: 650
      NUM_USERS: 350
      NUM_ROUNDS: 20
      SYSTEM_PROMPT: 0
      CHAT_HISTORY: 20000
      ANSWER_LEN: 1000
      QPS: [0.7]
      USE_SHAREGPT: false

    # commonly used combinations:

    # long input long output:
    - NUM_USERS_WARMUP: 750
      NUM_USERS: 350
      NUM_ROUNDS: 20
      SYSTEM_PROMPT: 0
      CHAT_HISTORY: 20000
      ANSWER_LEN: 1000
      QPS: [0.7]
      USE_SHAREGPT: false
    # long input short output:
    - NUM_USERS_WARMUP: 20
      NUM_USERS: 15
      NUM_ROUNDS: 20
      SYSTEM_PROMPT: 1000
      CHAT_HISTORY: 20000
      ANSWER_LEN: 100
      QPS: [0.1]
      USE_SHAREGPT: false

    # short input short output:
    - NUM_USERS_WARMUP: 400
      NUM_USERS: 320
      NUM_ROUNDS: 20
      SYSTEM_PROMPT: 0
      CHAT_HISTORY: 256
      ANSWER_LEN: 20
      QPS: [15]
      USE_SHAREGPT: false

  Mooncake:
    - NUM_ROUNDS: 20
      SYSTEM_PROMPT: 0
      CHAT_HISTORY: 256
      ANSWER_LEN: 20
      QPS: [1]

  Agentic:
    - NUM_USERS_WARMUP: 100
      NUM_AGENTS: 10
      NUM_ROUNDS: 20
      SYSTEM_PROMPT: 0
      CHAT_HISTORY: 256
      ANSWER_LEN: 20
      NEW_USER_INTERVALS: [1]


