Name: <NAME_OF_BENCHMARK_SUITE> # e.g. layerwise, routing, pd, etc.

# 2-serving-engines/
Serving:
  # Choose one or more of the following. You can have multiple of each as well.
  # NOTE: SGLang and RayServe require Local-Flat infrastructure in run-bench.yaml

  # use the latest helm repository from production-stack
  - Helm-ProductionStack:
      # this will start in the `2-serving-engines/helm-production-stack/helm_configurations/` directory
      # examples:
      # - helmConfigSelection: routing/1-kvaware.yaml
      # - helmConfigSelection: basic/simple-vllm.yaml
      helmConfigSelection: <NAME_OF_HELM_CONFIG_FILENAME>

      # You do NOT need to specify hf_token if you are using LMCacheGKE
      hf_token: <YOUR_HF_TOKEN>
      # Please make sure you are using the correct modelURL for the helm config you are using so that the workload generator can select the correct model
      modelURL: <MODEL_USED_IN_HELM_CONFIG>
      # API type: "completions" or "chat" (default: "completions")
      apiType: completions  # or "chat"

  # Any novel lmcache or production stack features can be tested here (before they are helm released)
  - Direct-ProductionStack:
      # this will start in the `2-serving-engines/direct-production-stack/kubernetes_configurations/` directory
      # generally this will be some modification after directly helm rendering the producttion stack helm chart
      # examples:
      # - kubernetesConfigSelection: routing/lmcache_roundrobin.yaml
      # - kubernetesConfigSelection: routing/lmcache_random.yaml
      # - kubernetesConfigSelection: routing/lmcache_roundrobin_with_prefix_caching.yaml
      kubernetesConfigSelection: <NAME_OF_K8S_CONFIG_FILENAME>

      # You do NOT need to specify hf_token if you are using LMCacheGKE
      hf_token: <YOUR_HF_TOKEN>
      # Please make sure you are using the correct modelURL for the kubernetes config you are using so that the workload generator can select the correct model
      modelURL: <MODEL_USED_IN_K8S_CONFIG>
      # API type: "completions" or "chat" (default: "completions")
      apiType: completions  # or "chat"

  # Option 3: SGLang (Local-Flat infrastructure ONLY)
  - SGLang:
      scriptName: comparison-baseline.sh  # Script in 2-serving-engines/sglang/
      modelURL: meta-llama/Llama-3.1-8B-Instruct
      # API type: "completions" or "chat" (default: "completions")
      apiType: completions  # or "chat"
      # NOTE: SGLang requires Local-Flat infrastructure. 
      # The script reads HF_TOKEN directly from environment variable.

  # Option 4: RayServe (Local-Flat infrastructure ONLY)
  - RayServe:
      scriptName: comparison-baseline.py  # Python script in 2-serving-engines/rayserve/
      acceleratorType: H100  # e.g., H100, A100, V100, T4
      modelURL: meta-llama/Llama-3.1-70B-Instruct
      # API type: "completions" or "chat" (default: "completions")
      apiType: completions  # or "chat"
      # NOTE: RayServe requires Local-Flat infrastructure.
      # The script reads HF_TOKEN directly from environment variable.
      # Example: ./choose-and-deploy.sh comparison-baseline.py H100

  # Option 5: Dynamo (Local-Flat infrastructure ONLY)
  - Dynamo:
      # Configuration file from 2-serving-engines/dynamo/dynamo_configurations/
      configSelection: comparison-baseline.yaml
      modelURL: meta-llama/Llama-3.1-8B-Instruct
      # API type: "completions" or "chat" (default: "completions")
      # Use "chat" for baselines that only support chat completions API
      apiType: completions  # or "chat"
      # NOTE: Dynamo requires Local-Flat infrastructure.
      # The script reads HF_TOKEN directly from environment variable.

  # Option 6: LLM-D (LocalMinikube infrastructure ONLY)
  - LLM-D:
      # Configuration file from 2-serving-engines/llm-d/llmd_configurations/
      configSelection: debug-deployment.yaml
      modelURL: meta-llama/Llama-3.1-8B-Instruct
      hf_token: <YOUR_HF_TOKEN>
      # NOTE: LLM-D requires LocalMinikube infrastructure.
      # The script reads HF_TOKEN directly from environment variable.

  # Option 7: Flat (Local-Flat infrastructure ONLY)
  - Flat:
      # Configuration script from 2-serving-engines/flat/
      # Available configurations: basic-vllm/run-llama8B.sh, basic-lmcache/run-llama8B.sh
      configSelection: basic-vllm/run-llama8B.sh  # or basic-lmcache/run-llama8B.sh
      modelURL: meta-llama/Llama-3.1-8B-Instruct
      # API type: "completions" or "chat" (default: "completions")
      apiType: completions  # or "chat"
      # NOTE: Flat baseline requires Local-Flat infrastructure.
      # The script reads HF_TOKEN directly from environment variable.
      # Examples:
      # - configSelection: basic-vllm/run-llama8B.sh     # Simple vLLM deployment
      # - configSelection: basic-lmcache/run-llama8B.sh  # vLLM with LMCache integration

  # Multiple Flat baselines for comparison (each will be deployed and tested separately):
  # - Flat:
  #     configSelection: basic-vllm/run-llama8B.sh
  #     modelURL: meta-llama/Llama-3.1-8B-Instruct
  #     apiType: completions
  # - Flat:
  #     configSelection: basic-lmcache/run-llama8B.sh
  #     modelURL: meta-llama/Llama-3.1-8B-Instruct
  #     apiType: completions

# 3-workloads/
Workload:
  # Multiple workloads can be specified and they will all be run.

  ShareGPT:
    - LIMIT: 1000
      MIN_ROUNDS: 10
      START_ROUND: 0
      QPS: [1.34, 2]

  LMCacheSynthetic:
    - NUM_USERS_WARMUP: 650
      NUM_USERS: 350
      NUM_ROUNDS: 20
      SYSTEM_PROMPT: 0 # shared between users
      CHAT_HISTORY: 20000 # unique for each user
      ANSWER_LEN: 1000
      QPS: [0.7]
      USE_SHAREGPT: false

    # commonly used combinations:

    # long input long output:
    - NUM_USERS_WARMUP: 750
      NUM_USERS: 350
      NUM_ROUNDS: 20
      SYSTEM_PROMPT: 0
      CHAT_HISTORY: 20000
      ANSWER_LEN: 1000
      QPS: [0.7]
      USE_SHAREGPT: false
    # long input short output:
    - NUM_USERS_WARMUP: 20
      NUM_USERS: 15
      NUM_ROUNDS: 20
      SYSTEM_PROMPT: 1000
      CHAT_HISTORY: 20000
      ANSWER_LEN: 100
      QPS: [0.1]
      USE_SHAREGPT: false

    # short input short output:
    - NUM_USERS_WARMUP: 400
      NUM_USERS: 320
      NUM_ROUNDS: 20
      SYSTEM_PROMPT: 0
      CHAT_HISTORY: 256
      ANSWER_LEN: 20
      QPS: [15]
      USE_SHAREGPT: false

  # TraceReplayer - Replays conversation traces with deterministic prompt generation
  # Supports two modes: timed replay (with SPEED_UP) OR QPS-controlled replay
  TraceReplayer:
    # Full trace replay mode - replays entire trace (DURATION automatically calculated)
    - TRACE_FILE: traces/gmi_trace.jsonl        # Main GMI trace (default)
      # START_TIME: 0                           # Optional: defaults to 0 
      # DURATION: full                          # Optional: 'full' or omit entirely for full trace
      # SPEED_UP: 1.0                           # Optional: defaults to 1.0 (real-time)
      PRESERVE_TIMING: true                     # Preserve original timestamps
      
    # Alternative ways to specify full trace duration
    - TRACE_FILE: traces/gmi_trace.jsonl
      DURATION: full                            # Explicit 'full' keyword
      PRESERVE_TIMING: true
      SPEED_UP: 2.0                             # 2x faster than original
      MAX_DELAY: 10.0                           # Cap delays at 10 seconds (useful for testing production traces)
      
    # High-speed testing of production traces with delay cap
    - TRACE_FILE: traces/gmi_trace.jsonl
      PRESERVE_TIMING: true
      SPEED_UP: 1000.0                          # 1000x faster (24 hours becomes ~86 seconds)
      MAX_DELAY: 5.0                            # Cap any delay at 5 seconds max
      
    # Timed replay mode - preserves original timestamps from trace with specific duration
    - TRACE_FILE: traces/gmi_trace.jsonl        
      START_TIME: 0                             # Start time in seconds (relative to trace start)
      DURATION: 60                              # Duration to replay in seconds
      PRESERVE_TIMING: true                     # Preserve original timestamps
      SPEED_UP: 1.0                             # Speed factor (1.0 = real-time, 2.0 = 2x faster, 10.0 = 10x faster)
      
    # QPS-controlled replay mode - ignores original timing, uses fixed request rate
    - TRACE_FILE: traces/gmi_trace.jsonl
      START_TIME: 0
      DURATION: 120                             # Longer duration for more requests
      PRESERVE_TIMING: false                    # Use QPS control instead of timestamps
      QPS: [1.0, 2.0, 5.0]                    # Request rates to test (NOTE: SPEED_UP ignored)
      
    # Example: Replay specific time window from trace (accelerated)
    - TRACE_FILE: traces/gmi_trace.jsonl
      START_TIME: 300                           # Start at 5 minutes into trace
      DURATION: 180                             # Replay 3 minutes worth
      PRESERVE_TIMING: true
      SPEED_UP: 2.0                             # 2x faster than original
      
    # Example: Using alternative trace file with full duration
    - TRACE_FILE: traces/mooncake_trace.jsonl   # Alternative conversation trace
      PRESERVE_TIMING: true                     # Full trace with original timing
      SPEED_UP: 1.0



  Agentic:
    - NUM_USERS_WARMUP: 100
      NUM_AGENTS: 10
      NUM_ROUNDS: 20
      SYSTEM_PROMPT: 0
      CHAT_HISTORY: 256
      ANSWER_LEN: 20
      NEW_USER_INTERVALS: [1]

  Random:
    # Random workload generates completely random prompts with no shared prefix
    # This is ideal for testing store-heavy workloads in LMCache
    - NUM_USERS: 100
      NUM_ROUNDS: 10
      PROMPT_LEN: 200  # Length of random prompts (in words)
      ANSWER_LEN: 100  # Length of the answer (max tokens)
      QPS: [1.0, 2.0]

    # commonly used combinations:

    # short random prompts:
    - NUM_USERS: 50
      NUM_ROUNDS: 15
      PROMPT_LEN: 50
      ANSWER_LEN: 50
      QPS: [5.0]

    # long random prompts:
    - NUM_USERS: 200
      NUM_ROUNDS: 20
      PROMPT_LEN: 500
      ANSWER_LEN: 200
      QPS: [0.5]

  VLLMBenchmark:
    # Super minimal configuration - only required parameters
    - BACKEND: vllm
      DATASET_NAME: random
      NUM_PROMPTS: 100
      REQUEST_RATES: [1.0]

    # VLLMBenchmark workload uses the vLLM benchmark serving script
    # This provides comprehensive benchmarking capabilities with various datasets
    - BACKEND: vllm  # Backend type: vllm, openai, etc.
      DATASET_NAME: random  # Dataset: sharegpt, random, sonnet, hf, custom
      DATASET_PATH: ""  # Path to dataset file (leave empty for built-in datasets)
      NUM_PROMPTS: 1000  # Number of prompts to process
      REQUEST_RATES: [1.0, 2.0, 5.0]  # Request rates to test (requests per second)

      # Optional sampling parameters
      TEMPERATURE: 0.0  # Temperature for sampling (0.0 for greedy)
      TOP_P: 0.9  # Top-p sampling parameter
      TOP_K: 50   # Top-k sampling parameter
      MAX_TOKENS: 256  # Maximum tokens to generate

      # Optional benchmark parameters
      BURSTINESS: 1.0  # Burstiness factor (1.0 = Poisson process)
      SEED: 0  # Random seed for reproducibility
      DISABLE_TQDM: true  # Disable progress bar
      IGNORE_EOS: false  # Ignore end-of-sequence tokens

      # For Random dataset:
      RANDOM_INPUT_LEN: 1024   # Input length for random prompts
      RANDOM_OUTPUT_LEN: 128   # Output length for random prompts
      RANDOM_RANGE_RATIO: 0.0  # Range ratio for length variation

    # Example with Sonnet dataset (requires sonnet.txt file)
    - BACKEND: vllm
      DATASET_NAME: sonnet
      DATASET_PATH: "3-workloads/vllm-benchmark-serving/sonnet.txt"  # Path to sonnet file
      NUM_PROMPTS: 500
      REQUEST_RATES: [0.5, 1.0]
      SONNET_INPUT_LEN: 550    # Input length for sonnet prompts
      SONNET_OUTPUT_LEN: 150   # Output length for sonnet prompts
      SONNET_PREFIX_LEN: 200   # Prefix length for sonnet prompts
      TEMPERATURE: 0.7
      SEED: 42

    # Example with HuggingFace dataset (MT-Bench is non-multimodal)
    - BACKEND: vllm
      DATASET_NAME: hf
      DATASET_PATH: "philschmid/mt-bench"  # HuggingFace dataset ID
      NUM_PROMPTS: 200
      REQUEST_RATES: [1.0, 2.0]
      TEMPERATURE: 0.0
      MAX_TOKENS: 256

  StrictSynthetic:
    # Strict Synthetic workload for measuring benchmarks in a more controlled way
    # QPS is calculated as NUM_CONCURRENT_USERS / TIME_BETWEEN_REQUESTS_PER_USER
    # User ID is automatically included in requests (enabled by default)
    - NUM_CONCURRENT_USERS: 10  # Number of concurrent users in the system
      NUM_ROUNDS_PER_USER: 5    # Number of rounds per user
      TIME_BETWEEN_REQUESTS_PER_USER: [10, 20, 30]  # Time between requests per user (seconds)
      SHARED_SYSTEM_PROMPT_LEN: 100   # Length of shared system prompt (tokens)
      FIRST_PROMPT_LEN: 200           # Length of first prompt (tokens)
      FOLLOW_UP_PROMPTS_LEN: 100      # Length of follow-up prompts (tokens)
      ANSWER_LEN: 150                 # Length of answers (tokens)
      KV_REUSE_RATIO: 1.0             # Ratio of conversation history reused between requests (0.0-1.0, default: 1.0)

    # Example with different user concurrency levels
    - NUM_CONCURRENT_USERS: 5
      NUM_ROUNDS_PER_USER: 10
      TIME_BETWEEN_REQUESTS_PER_USER: [5, 15]
      SHARED_SYSTEM_PROMPT_LEN: 50
      FIRST_PROMPT_LEN: 300
      FOLLOW_UP_PROMPTS_LEN: 150
      ANSWER_LEN: 200
      KV_REUSE_RATIO: 0.8             # 80% of conversation history reused, 20% randomized

    # Example for high-throughput testing
    - NUM_CONCURRENT_USERS: 20
      NUM_ROUNDS_PER_USER: 3
      TIME_BETWEEN_REQUESTS_PER_USER: [2, 5]
      SHARED_SYSTEM_PROMPT_LEN: 0
      FIRST_PROMPT_LEN: 100
      FOLLOW_UP_PROMPTS_LEN: 50
      ANSWER_LEN: 100
      KV_REUSE_RATIO: 1.0             # Full conversation history reuse (default behavior)

    # Example for testing KV cache miss scenarios
    - NUM_CONCURRENT_USERS: 8
      NUM_ROUNDS_PER_USER: 4
      TIME_BETWEEN_REQUESTS_PER_USER: [10]
      SHARED_SYSTEM_PROMPT_LEN: 200
      FIRST_PROMPT_LEN: 500
      FOLLOW_UP_PROMPTS_LEN: 200
      ANSWER_LEN: 100
      KV_REUSE_RATIO: 0.0             # No conversation history reuse (completely randomized)




# ============================================================================
# EXAMPLE: Multiple Baselines for Comprehensive Comparison
# ============================================================================
# 
# You can specify multiple baselines of ANY TYPE and the framework will:
# 1. Deploy Baseline 1 → Run all workloads → Collect results
# 2. Deploy Baseline 2 → Run all workloads → Collect results  
# 3. Deploy Baseline 3 → Run all workloads → Collect results
# 4. Generate comparison visualizations across all baselines
#
# Example comprehensive comparison spec:
#
# Serving:
#   # Multiple Helm-ProductionStack baselines
#   - Helm-ProductionStack:
#       helmConfigSelection: basic/simple-vllm.yaml
#       hf_token: <YOUR_HF_TOKEN>
#       modelURL: meta-llama/Llama-3.1-8B-Instruct
#   - Helm-ProductionStack:
#       helmConfigSelection: routing/1-kvaware.yaml
#       hf_token: <YOUR_HF_TOKEN>
#       modelURL: meta-llama/Llama-3.1-8B-Instruct
#
#   # Multiple SGLang baselines (Local-Flat only)
#   - SGLang:
#       scriptName: comparison-baseline.sh
#       modelURL: meta-llama/Llama-3.1-8B-Instruct
#   - SGLang:
#       scriptName: another-config.sh
#       modelURL: meta-llama/Llama-3.1-70B-Instruct
#
#   # Multiple Dynamo baselines (Local-Flat only)
#   - Dynamo:
#       configSelection: comparison-baseline.yaml
#       modelURL: meta-llama/Llama-3.1-8B-Instruct
#   - Dynamo:
#       configSelection: optimized-config.yaml
#       modelURL: meta-llama/Llama-3.1-8B-Instruct
#
#   # Multiple Flat baselines (Local-Flat only)
#   - Flat:
#       configSelection: basic-vllm/run-llama8B.sh
#       modelURL: meta-llama/Llama-3.1-8B-Instruct
#   - Flat:
#       configSelection: basic-lmcache/run-llama8B.sh
#       modelURL: meta-llama/Llama-3.1-8B-Instruct
#
# This will generate unique keys like:
# - helm_basic_simple_vllm
# - helm_routing_1_kvaware  
# - sglang_comparison_baseline
# - sglang_another_config
# - dynamo_comparison_baseline
# - dynamo_optimized_config
# - flat_basic_vllm_run_llama8B
# - flat_basic_lmcache_run_llama8B
# ============================================================================


