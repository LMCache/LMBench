Name: daily-4-A100-benchmarking

# Suggested Infrastructure (in run-bench.yaml)
# 1-infrastructure:
#   Location: LMCacheGKE
#   numClusterGPUs: 1
#   A100_VRAM: 40
#   OR
#   Location: LocalMinikube

Serving:
  - Helm-ProductionStack:
      helmConfigSelection: daily/4-no-lmcache.yaml
      hf_token: <YOUR_HF_TOKEN>
      modelURL: meta-llama/Llama-3.1-8B-Instruct
  - Helm-ProductionStack:
      helmConfigSelection: daily/4-roundrobin.yaml
      hf_token: <YOUR_HF_TOKEN>
      modelURL: meta-llama/Llama-3.1-8B-Instruct
  - Helm-ProductionStack:
      helmConfigSelection: daily/4-session.yaml
      hf_token: <YOUR_HF_TOKEN>
      modelURL: meta-llama/Llama-3.1-8B-Instruct

Workload:
  # ShareGPT:
  #   # High memory pressure ShareGPT workload to trigger LMCache KV offloading:
  #   - LIMIT: 1000
  #     MIN_ROUNDS: 10
  #     START_ROUND: 0
  #     QPS: [2, 4, 6, 8]

  LMCacheSynthetic:
    # High memory pressure workload to trigger LMCache KV offloading:
    - NUM_USERS_WARMUP: 60
      NUM_USERS: 60
      NUM_ROUNDS: 15
      SYSTEM_PROMPT: 1000
      CHAT_HISTORY: 20000
      ANSWER_LEN: 100
      QPS: [0.2, 0.5, 0.8, 1.2, 1.6, 2, 3]
      USE_SHAREGPT: false