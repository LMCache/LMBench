Infrastructure:
  # Location: NoBench # when pushing changes that should not run any workflows
  # Location: LocalMinikube
  Location: LMCacheGKE
  numClusterGPUs: 8

Serving:
  Baseline: Helm-ProductionStack
  Helm-ProductionStack:
    vLLM-Version: 1
    useLMCache: false
    cpuSize: 60
    modelURL: meta-llama/Llama-3.1-70B-Instruct
    enablePrefixCaching: false # vllm v1 specific only (no prefix caching in v0)
    replicaCount: 1
    numGPUs: 8
    numCPUs: 10
    tensorParallelSize: 8
    hf_token: <YOUR_HF_TOKEN>
    maxModelLen: 16384

  # Baseline: Latest-ProductionStack
  # Latest-ProductionStack:
  #   useLMCache: true
  #   cpuSize: 60
  #   modelURL: meta-llama/Llama-3.1-70B-Instruct
  #   enablePrefixCaching: false
  #   replicaCount: 4
  #   numGPUs: 2
  #   numCPUs: 4
  #   tensorParallelSize: 2
  #   hf_token: <YOUR_HF_TOKEN>
  #   maxModelLen: 16384

Workload:

  ShareGPT:
    - LIMIT: 1000
      MIN_ROUNDS: 10
      START_ROUND: 0
      QPS: [1.34, 2]

  LMCacheSynthetic:
    # long input long output:
    - NUM_USERS_WARMUP: 750
      NUM_USERS: 350
      NUM_ROUNDS: 20
      SYSTEM_PROMPT: 0
      CHAT_HISTORY: 20000
      ANSWER_LEN: 1000
      QPS: [0.7]
      USE_SHAREGPT: false

    # long input short output:
    - NUM_USERS_WARMUP: 20
      NUM_USERS: 15
      NUM_ROUNDS: 20
      SYSTEM_PROMPT: 1000
      CHAT_HISTORY: 20000
      ANSWER_LEN: 100
      QPS: [0.1]
      USE_SHAREGPT: false
    # short input short output:
    - NUM_USERS_WARMUP: 400
      NUM_USERS: 320
      NUM_ROUNDS: 20
      SYSTEM_PROMPT: 0
      CHAT_HISTORY: 256
      ANSWER_LEN: 20
      QPS: [15]
      USE_SHAREGPT: false

  # Mooncake:
  # - NUM_ROUNDS: 20
  #   SYSTEM_PROMPT: 0
  #   CHAT_HISTORY: 256
  #   ANSWER_LEN: 20
  #   QPS: [1]

  # Agentic:
  # - NUM_USERS_WARMUP: 100
  #   NUM_AGENTS: 10
  #   NUM_ROUNDS: 20
  #   SYSTEM_PROMPT: 0
  #   CHAT_HISTORY: 256
  #   ANSWER_LEN: 20
  #   QPS: [1]