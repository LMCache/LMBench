Infrastructure:
  Location: LocalMinikube

Serving:
  Baseline: KubeRay
  KubeRay:
    modelURL: meta-llama/Llama-3.1-8B-Instruct
    hf_token: <YOUR_HF_TOKEN>
    headGPUs: 1
    workerGPUs: 1
    numWorkers: 1
    headCPUs: 2
    workerCPUs: 2
    headMemory: "4G"
    workerMemory: "4G"
    modelCacheSize: "20Gi"

Workload:
  # Keeping this simple for testing
  Mooncake:
    - NUM_ROUNDS: 2
      SYSTEM_PROMPT: 0
      CHAT_HISTORY: 256
      ANSWER_LEN: 20
      QPS: [0.5]