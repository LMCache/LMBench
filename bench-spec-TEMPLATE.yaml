Infrastructure:
  # Option 0: None (when pushing changes that should not run any workflows)
  Location: NoBench

  # Option 1: Minikube (local clone of this repo)
  Location: LocalMinikube

  # Option 2: LMCacheGKE (workflow will run on LMCache GPU runner)
  Location: LMCacheGKE
  numClusterGPUs: 1
  A100_VRAM: 40 # either 40 or 80 (80 is difficult to obtain on GCP so will run as "spot" and may fail)
  # 1 GPU -> 12 vCPUs, 85GB RAM
  # 2 GPUs -> 24 vCPUs, 170GB RAM
  # 4 GPUs -> 48 vCPUs, 340GB RAM
  # 8 GPUs -> 96 vCPUs, 680GB RAM

  # NOTE: from experience, 4x 40GB A100s with TP 4 cannot run a llama 3.1 70B model with 8192 max_seq_len
  # so we recommend using 8x 40GB A100s with TP 8 for llama 3.1 70B
  # or 2x 80GB A100s with TP 2 for llama 3.1 70B

Serving:
  # Option 1: Helm-ProductionStack (Uses the latest helm repository from production-stack)
  # PLEASE make sure that replicaCount x numGPUs <= numClusterGPUs
  Baseline: Helm-ProductionStack
  Helm-ProductionStack:
    vLLM-Version: 0 # vllm v0 or v1
    enablePrefixCaching: false # vllm v1 specific only (no prefix caching in v0)
    useLMCache: false # true or false
    modelURL: meta-llama/Llama-3.1-8B-Instruct
    replicaCount: 1 # PLEASE make sure that replicaCount x numGPUs <= numClusterGPUs
    numGPUs: 1 # PLEASE make sure that replicaCount x numGPUs <= numClusterGPUs
    numCPUs: 4 # PLEASE look at the vCPU limits in the comment above (try to keep 12 or below)
    tensorParallelSize: 1 # please make sure tensorParallelSize <= numGPUs (this is the number of GPUs per replica)
    hf_token: <YOUR_HF_TOKEN> # do NOT modify if you are using LMCacheGKE (keep as <YOUR_HF_TOKEN>). This is only needed for LocalMinikube
    maxModelLen: 16384

  # Option 2: Direct-ProductionStack
  # This will allow us to test various configurations using production stack directly (choosing various images of lmcache and routers):
  # PD vs Non-PD
  # KV Cache Aware Routing
  # Prefix Aware Routing
  # Layer by layer pipelining
  # Any novel lmcache or production stack features can be tested here with their docker images
  Baseline: Direct-ProductionStack
  Direct-ProductionStack:
    kubernetesConfigSelection: <NAME_OF_K8S_CONFIG_FILENAME> # please end with .yaml
      # this will start in the `2-serving-engines/direct-production-stack/kubernetes_configurations/` directory
      # generally this will be some modification after directly helm rendering the producttion stack helm chart
    hf_token: <YOUR_HF_TOKEN> # do NOT modify if you are using LMCacheGKE (keep as <YOUR_HF_TOKEN>). This is only needed for LocalMinikube
    modelURL: <MODEL_USED_IN_K8S_CONFIG>
      # even though the model is hardcoded into the kubernetes config, we still need to specify it here
      # because the workload stage requires knowledge of the modelURL to tokenize and send requests

  # Option 3: SGLang
  Baseline: SGLang
  SGLang:
    modelURL: meta-llama/Llama-3.1-8B-Instruct # specify your model
    hf_token: <YOUR_HF_TOKEN> # do NOT modify if you are using LMCacheGKE (keep as <YOUR_HF_TOKEN>). This is only needed for LocalMinikube
    replicaCount: 1 # number of replicas to run
    numGPUs: 1 # number of GPUs per replica
    numCPUs: 10 # number of CPUs per replica
    requestMemory: "50Gi" # memory request per replica
    shmSize: "20Gi" # shared memory size
    cacheSize: "50Gi" # size of the HuggingFace cache volume
    contextLength: 32768 # context length for the model
    tensorParallelSize: 1 # tensor parallel size for model distribution across GPUs

  # Option 4: Dynamo
  Baseline: Dynamo
  Dynamo:
    # Coming soon...

Workload:
  # Multiple workloads can be specified and they will all be run.
  ShareGPT:
    - LIMIT: 1000
      MIN_ROUNDS: 10
      START_ROUND: 0
      QPS: [1.34, 2]

  LMCacheSynthetic:
    - NUM_USERS_WARMUP: 650
      NUM_USERS: 350
      NUM_ROUNDS: 20
      SYSTEM_PROMPT: 0
      CHAT_HISTORY: 20000
      ANSWER_LEN: 1000
      QPS: [0.7]
      USE_SHAREGPT: false

    # commonly used combinations:

    # long input long output:
    - NUM_USERS_WARMUP: 750
      NUM_USERS: 350
      NUM_ROUNDS: 20
      SYSTEM_PROMPT: 0
      CHAT_HISTORY: 20000
      ANSWER_LEN: 1000
      QPS: [0.7]
      USE_SHAREGPT: false
    # long input short output:
    - NUM_USERS_WARMUP: 20
      NUM_USERS: 15
      NUM_ROUNDS: 20
      SYSTEM_PROMPT: 1000
      CHAT_HISTORY: 20000
      ANSWER_LEN: 100
      QPS: [0.1]
      USE_SHAREGPT: false

    # short input short output:
    - NUM_USERS_WARMUP: 400
      NUM_USERS: 320
      NUM_ROUNDS: 20
      SYSTEM_PROMPT: 0
      CHAT_HISTORY: 256
      ANSWER_LEN: 20
      QPS: [15]
      USE_SHAREGPT: false

  Mooncake:
    - NUM_ROUNDS: 20
      SYSTEM_PROMPT: 0
      CHAT_HISTORY: 256
      ANSWER_LEN: 20
      QPS: [1]

  Agentic:
    - NUM_USERS_WARMUP: 100
      NUM_AGENTS: 10
      NUM_ROUNDS: 20
      SYSTEM_PROMPT: 0
      CHAT_HISTORY: 256
      ANSWER_LEN: 20
      NEW_USER_INTERVALS: [1]


