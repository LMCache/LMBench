Infrastructure:
  # Option 0: None (when pushing changes that should not run any workflows)
  Location: NoBench

  # Option 1: Minikube (local clone of this repo)
  Location: LocalMinikube

  # Option 2: LMCacheGKE (workflow will run on LMCache GPU runner)
  Location: LMCacheGKE
  numClusterGPUs: 1
  # numClusterGPUs: Only 1, 2, 4, 8 (A100 40GB) or 16 (A100 80GB) GPUs are supported.
  # 1 GPU -> 12 vCPUs, 85GB memory
  # 2 GPUs -> 24 vCPUs, 170GB memory
  # 4 GPUs -> 48 vCPUs, 340GB memory
  # 8 GPUs -> 96 vCPUs, 680GB memory
  # 16 GPUs -> 128 vCPUs, 1920GB memory

Serving:
  # Option 1: ProductionStack
  # PLEASE make sure that replicaCount x numGPUs <= numClusterGPUs
  Baseline: ProductionStack
  ProductionStack:
    vLLM-Version: 0 # vllm v0 or v1
    enablePrefixCaching: false # vllm v1 specific only (no prefix caching in v0)
    useLMCache: false # true or false
    modelURL: meta-llama/Llama-3.1-8B-Instruct
    replicaCount: 1 # PLEASE make sure that replicaCount x numGPUs <= numClusterGPUs
    numGPUs: 1 # PLEASE make sure that replicaCount x numGPUs <= numClusterGPUs
    numCPUs: 4 # PLEASE look at the vCPU limits in the comment above (try to keep 12 or below)
    tensorParallelSize: 1 # please make sure tensorParallelSize <= numGPUs (this is the number of GPUs per replica)
    hf_token: <YOUR_HF_TOKEN>
    maxModelLen: 16384

  # Option 2: SGLang
  Baseline: SGLang
  SGLang:
    modelURL: meta-llama/Llama-3.1-70B-Instruct # specify your model
    hf_token: <YOUR_HF_TOKEN>
    # Coming soon...

  # Option 3: Dynamo
  Baseline: Dynamo
  Dynamo:
    # Coming soon...

Workload:
  # Multiple workloads can be specified and they will all be run.
  ShareGPT:
    - LIMIT: 1000
      MIN_ROUNDS: 10
      START_ROUND: 0
      QPS: [1.34, 2]

  LMCacheSynthetic:
    - NUM_USERS_WARMUP: 650
      NUM_USERS: 350
      NUM_ROUNDS: 20
      SYSTEM_PROMPT: 0
      CHAT_HISTORY: 20000
      ANSWER_LEN: 1000
      QPS: [0.7]
      USE_SHAREGPT: false

    # commonly used combinations:

    # long input long output:
    - NUM_USERS_WARMUP: 750
      NUM_USERS: 350
      NUM_ROUNDS: 20
      SYSTEM_PROMPT: 0
      CHAT_HISTORY: 20000
      ANSWER_LEN: 1000
      QPS: [0.7]
      USE_SHAREGPT: false
    # long input short output:
    - NUM_USERS_WARMUP: 20
      NUM_USERS: 15
      NUM_ROUNDS: 20
      SYSTEM_PROMPT: 1000
      CHAT_HISTORY: 20000
      ANSWER_LEN: 100
      QPS: [0.1]
      USE_SHAREGPT: false

    # short input short output:
    - NUM_USERS_WARMUP: 400
      NUM_USERS: 320
      NUM_ROUNDS: 20
      SYSTEM_PROMPT: 0
      CHAT_HISTORY: 256
      ANSWER_LEN: 20
      QPS: [15]
      USE_SHAREGPT: false

  Mooncake:
    - NUM_ROUNDS: 20
      SYSTEM_PROMPT: 0
      CHAT_HISTORY: 256
      ANSWER_LEN: 20
      QPS: [1]

    # commonly used combinations:
    # NOTE: max model len here needs to be >= 23692
    # NUM_ROUNDS: 20
    # SYSTEM_PROMPT: 0
    # CHAT_HISTORY: 256
    # ANSWER_LEN: 20
    # QPS: [1]

  Agentic:
    - NUM_USERS_WARMUP: 100
      NUM_AGENTS: 10
      NUM_ROUNDS: 20
      SYSTEM_PROMPT: 0
      CHAT_HISTORY: 256
      ANSWER_LEN: 20
      QPS: [1]


