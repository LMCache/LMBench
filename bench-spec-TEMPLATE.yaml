Infrastructure:
  # Option 1: Minikube (local clone of this repo)
  Location: Minikube

  # Option 2: LMCacheGKE (workflow will run on LMCache GPU runner)
  Location: LMCacheGKE
  numClusterGPUs: 1
  # numClusterGPUs: Only 1, 2, 4, 8 (A100 40GB) or 16 (A100 80GB) GPUs are supported.

Serving:
  # Option 1: ProductionStack
  Baseline: ProductionStack
  ProductionStack:
    vLLM-Version: 0
    useLMCache: false
    modelURL: meta-llama/Llama-3.1-8B-Instruct
    replicaCount: 1
    numGPUs: 1
    tensorParallelSize: 1
    hf_token: <YOUR_HF_TOKEN>
    maxModelLen: 16384

  # Option 2: SGLang
  Baseline: SGLang
  SGLang:
    modelURL: meta-llama/Llama-3.1-70B-Instruct
    hf_token: <YOUR_HF_TOKEN>
    # TODO

  # Option 3: Dynamo
  Baseline: Dynamo
  Dynamo:
    # TODO

Workload:
  # Multiple workloads can be specified and they will all be run.
  ShareGPT
    LIMIT: 1000
    MIN_ROUNDS: 10
    START_ROUND: 0
    QPS: [1.34, 2]

  LMCacheSynthetic
    # TODO

  Agentic
    # TODO

  Mooncake
    # TODO


