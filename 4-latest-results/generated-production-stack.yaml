routerSpec:
  resources:
    limits:
      cpu: '2'
      memory: 8G
    requests:
      cpu: '2'
      memory: 8G
  routingLogic: roundrobin
servingEngineSpec:
  modelSpec:
  - hf_token: <YOUR_HF_TOKEN>
    lmcacheConfig:
      cpuOffloadingBufferSize: '60'
      enabled: false
    modelURL: meta-llama/Llama-3.1-8B-Instruct
    name: llama3
    pvcAccessMode:
    - ReadWriteOnce
    pvcStorage: 50Gi
    replicaCount: 1
    repository: lmcache/vllm-openai
    requestCPU: 10
    requestGPU: 1
    requestMemory: 50Gi
    shmSize: 20Gi
    tag: '2025-03-28'
    vllmConfig:
      dtype: bfloat16
      extraArgs:
      - --disable-log-requests
      - --swap-space
      - 0
      - --gpu-memory-utilization
      - '0.95'
      maxModelLen: 20000
      tensorParallelSize: 1
  runtimeClassName: ''
