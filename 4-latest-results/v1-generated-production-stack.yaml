routerSpec:
  repository: lmcache/lmstack-router
  resources:
    limits:
      cpu: '2'
      memory: 8G
    requests:
      cpu: '2'
      memory: 8G
  routingLogic: session
  sessionKey: x-user-id
  tag: benchmark
servingEngineSpec:
  modelSpec:
  - hf_token: <YOUR_HF_TOKEN>
    lmcacheConfig:
      cpuOffloadingBufferSize: '60'
      enabled: true
    modelURL: meta-llama/Llama-3.1-8B-Instruct
    name: llama3
    pvcAccessMode:
    - ReadWriteOnce
    pvcStorage: 50Gi
    replicaCount: 2
    repository: lmcache/vllm-openai
    requestCPU: 4
    requestGPU: 1
    requestMemory: 50Gi
    shmSize: 20Gi
    tag: latest
    vllmConfig:
      dtype: bfloat16
      enableChunkedPrefill: false
      enablePrefixCaching: false
      extraArgs:
      - --disable-log-requests
      - --swap-space
      - 0
      gpuMemoryUtilization: '0.95'
      maxModelLen: 16384
      tensorParallelSize: 1
  runtimeClassName: ''
