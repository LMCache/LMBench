Launched queries: 0, pending queries: 0, finished queries: 0

==================== Performance summary ======================
  QPS: 0.0000 reqs/s
  Processing speed: nan reqs/s
  Requests on-the-fly: 0
  Input tokens per second: nan tokens/s
  Output tokens per second: nan tokens/s
  Average generation throughput (per request): nan tokens/req/s
  Average TTFT: nans
  Average generation_time / generation_tokens (Inter-Token Latency): nan
  Time range: nan - nan (nans)
===============================================================


==================== bench-spec.yaml ======================
Infrastructure:
  # Location: NoBench # when pushing changes that should not run any workflows
  Location: LMCacheGKE
  numClusterGPUs: 2

Serving:
  Baseline: ProductionStack
  ProductionStack:
    vLLM-Version: 1
    useLMCache: true
    cpuSize: 60
    modelURL: meta-llama/Llama-3.1-8B-Instruct
    enablePrefixCaching: false # vllm v1 specific only (no prefix caching in v0)
    replicaCount: 2
    numGPUs: 1
    numCPUs: 4
    tensorParallelSize: 1
    maxModelLen: 16384

Workload:
  LMCacheSynthetic:
    # long input long output:
    - NUM_USERS_WARMUP: 750
      NUM_USERS: 350
      NUM_ROUNDS: 20
      SYSTEM_PROMPT: 0
      CHAT_HISTORY: 20000
      ANSWER_LEN: 1000
      QPS: [0.7]

    # long input short output:
    - NUM_USERS_WARMUP: 20
      NUM_USERS: 15
      NUM_ROUNDS: 20
      SYSTEM_PROMPT: 1000
      CHAT_HISTORY: 20000
      ANSWER_LEN: 100
      QPS: [0.1]

    # short input short output:
    - NUM_USERS_WARMUP: 400
      NUM_USERS: 320
      NUM_ROUNDS: 20
      SYSTEM_PROMPT: 0
      CHAT_HISTORY: 256
      ANSWER_LEN: 20
      QPS: [15]
===========================================================
